
import os
import json
import logging
import asyncio
from app.services.supabase_client import supabase_service
from app.services.log_stream import log_stream_manager
import traceback

from app.agents.google_researcher import GoogleResearcherAgent
from app.agents.matcher import MatcherAgent
from app.agents.applier import ApplierAgent

logger = logging.getLogger(__name__)

def update_research_status(user_id: int, resume_filename: str, status: str):
    """
    Updates the 'research_status' in the user's profile_data.
    States: IDLE, QUEUED, SEARCHING, COMPLETED, FAILED
    """
    try:
        response = supabase_service.client.table("profiles").select("profile_data").eq("user_id", user_id).execute()
        if not response.data:
            return

        current_data = response.data[0].get("profile_data") or {}
        if "research_status" not in current_data:
            current_data["research_status"] = {}

        current_data["research_status"][resume_filename] = {
            "status": status,
            "updated_at": str(os.times())
        }

        supabase_service.update_user_profile(user_id, {"profile_data": current_data})

    except Exception as e:
        logger.error(f"Failed to update research status: {e}")



async def check_cancellation(user_id: int, resume_filename: str):
    """Checks if the research task has been cancelled by the user."""
    try:
        response = supabase_service.client.table("profiles").select("profile_data").eq("user_id", user_id).execute()
        if response.data:
            data = response.data[0].get("profile_data", {})
            status_entry = data.get("research_status", {}).get(resume_filename, {})
            if status_entry.get("status") == "CANCEL_REQUESTED":
                return True
    except Exception:
        pass
    return False

async def run_research_pipeline(user_id: int, resume_filename: str, api_key: str, limit: int = 20, job_title: str = None, location: str = None, session_id: int = None):
    print(f"üïµÔ∏è Worker: Starting Research for {resume_filename} with limit {limit} (Type: Google)...")
    
    # Broadcast Function helper (Switching to Supabase Realtime broadcast via Service in future, 
    # but for now, we ensure we write critical logs to Chat History if not streaming locally)
    async def log(msg, type="log"):
        if session_id:
            await log_stream_manager.broadcast(str(session_id), msg, type=type)
            # Also save to DB for persistence if it's a major event
            if type in ["complete", "error", "warning"]:
                # We do NOT save every log to DB to avoid clutter, relying on Realtime
                pass

    await log(f"Starting research pipeline for {resume_filename}...")
    
    # Cloud Dispatch Check
    cloud_url = os.getenv("CLOUD_RUN_URL")
    if cloud_url and not os.getenv("IS_CLOUD_WORKER"):
        import httpx
        print(f"üöÄ Dispatching Research task to Cloud Worker: {cloud_url}")
        await log("Dispatching to Cloud Worker...")
        
        try:
            async with httpx.AsyncClient() as client:
                payload = {
                    "type": "research",
                    "user_id": user_id,
                    "resume_filename": resume_filename,
                    "api_key": api_key,
                    "limit": limit,
                    "job_title": job_title,
                    "location": location,
                    "session_id": session_id
                }
                headers = {"x-worker-secret": os.getenv("WORKER_SECRET", "")}
                resp = await client.post(f"{cloud_url}/api/worker/task", json=payload, headers=headers, timeout=10.0)
                if resp.status_code != 200:
                    print(f"‚ùå Cloud Dispatch Failed: {resp.text}")
                    await log("Cloud dispatch failed, running locally...", type="warning")
                else:
                    return # Successfully dispatched
        except Exception as e:
             print(f"‚ùå Cloud Dispatch Error: {traceback.format_exc()}")
             await log(f"Cloud dispatch error: {e}, running locally...", type="warning")

    update_research_status(user_id, resume_filename, "SEARCHING")

    try:
        # CANCELLATION CHECK
        if await check_cancellation(user_id, resume_filename): raise asyncio.CancelledError()

        # 1. Download Resume File
        remote_path = f"{user_id}/{resume_filename}"

        try:
            file_bytes = supabase_service.download_file(remote_path)
        except Exception as dl_error:
            print(f"‚ùå Failed to download resume: {dl_error}")
            update_research_status(user_id, resume_filename, "FAILED")
            return

        # Save temp for parsing
        await log("Downloading resume file...")
        import uuid
        tmp_id = str(uuid.uuid4())
        tmp_path = f"/tmp/{tmp_id}_{resume_filename}"

        with open(tmp_path, "wb") as f:
            f.write(file_bytes)

        # CANCELLATION CHECK
        if await check_cancellation(user_id, resume_filename): raise asyncio.CancelledError()

        # 2. Parse Resume (Dynamic)
        from app.utils.resume_parser import ResumeParser
        parser = ResumeParser(api_key=api_key)

        # Parse returns a JSON string, we need to load it
        await log("Parsing resume with Gemini 2.5 Flash...")
        parsed_json_str = await parser.parse_to_json(tmp_path)

        # Guard against Non-string return (e.g. None)
        if not parsed_json_str or not isinstance(parsed_json_str, str):
            print(f"‚ö†Ô∏è Parsing returned empty or invalid type: {type(parsed_json_str)}")
            profile_blob = {"raw_text": "Parsing Failed or Empty Response"}
        else:
            try:
                profile_blob = json.loads(parsed_json_str)
            except json.JSONDecodeError:
                # Fallback cleaning
                import re
                match = re.search(r'```json\n(.*?)\n```', parsed_json_str, re.DOTALL)
                if match:
                    profile_blob = json.loads(match.group(1))
                else:
                    profile_blob = {"raw_text": parsed_json_str} # Fallback

        # Clean up temp file
        if os.path.exists(tmp_path):
            try:
                os.remove(tmp_path)
            except:
                pass

        print(f"üìÑ Parsed Profile for {resume_filename}: {profile_blob.get('full_name', 'Unknown')}")

        # CANCELLATION CHECK
        if await check_cancellation(user_id, resume_filename): raise asyncio.CancelledError()

        # 3. Research
        print("üîé Using Google Verification Agent...")
        await log(f"Searching Google for top {limit} jobs (this may take a moment)...")
        researcher = GoogleResearcherAgent(api_key=api_key)
             
        # We use the DYNAMIC profile_blob here
        leads = await researcher.gather_leads(profile_blob, limit=limit, job_title=job_title, location=location)

        # Prefix query_source for UI identification
        prefix = "GOOGLE"
        for lead in leads:
            lead['query_source'] = f"{prefix}|{lead.get('query_source', 'Unknown')}"

        # CANCELLATION CHECK
        if await check_cancellation(user_id, resume_filename): raise asyncio.CancelledError()

        # 4. Match
        await log(f"Found {len(leads)} raw leads. analyzing matches with Matcher Agent...")
        matcher = MatcherAgent(api_key=api_key)
        # Fix: Use the requested limit, not hardcoded 10
        scored_matches = await matcher.filter_and_score_leads(leads, profile_blob, limit=limit)

        # 5. Save Results
        # Save to Storage as JSON (Legacy Backup) & DB
        results_filename = f"matches_{resume_filename}.json"

        # Serialize
        json_bytes = json.dumps(scored_matches, indent=2).encode('utf-8')

        supabase_service.upload_file(
            file_content=json_bytes,
            file_name=results_filename,
            user_id=user_id,
            content_type="application/json"
        )

        # Save to DB
        supabase_service.save_leads_bulk(user_id, resume_filename, scored_matches)

        print(f"‚úÖ Worker: Research Completed. Saved {len(scored_matches)} matches.")
        update_research_status(user_id, resume_filename, "COMPLETED")
        
        await log(f"Done! Found {len(scored_matches)} matches.", type="complete")
        if session_id:
            supabase_service.save_chat_message(session_id, "model", f"‚úÖ Research Complete! Found **{len(scored_matches)}** matches.\n\nCheck the **Jobs** tab or reload your dashboard.")

    except asyncio.CancelledError:
        print(f"üõë Worker: Research Cancelled for {resume_filename}")
        await log("Research cancelled by user.", type="error")
        if session_id:
            supabase_service.save_chat_message(session_id, "model", "üõë Research Cancelled by user.")
        update_research_status(user_id, resume_filename, "CANCELLED")
        
    except Exception as e:
        print(f"‚ùå Worker: Research Failed: {e}")
        traceback.print_exc()
        if session_id:
            supabase_service.save_chat_message(session_id, "model", f"‚ùå Research Failed: {e}")
            
        # Ensure we update status to FAILED so UI unblocks
        try:
            update_research_status(user_id, resume_filename, "FAILED")
        except Exception as status_err:
             print(f"‚ùå Failed to final update status: {status_err}")


async def run_applier_task(job_url: str, resume_path: str, user_profile: dict, api_key: str, resume_filename: str = None, execution_mode: str = "local", session_id: int = None, instructions: str = None):
    print(f"üöÄ Worker: Applying to {job_url} ...")
    
    async def log(msg, type="log"):
         if session_id: await log_stream_manager.broadcast(str(session_id), msg, type=type)

    if session_id:
        supabase_service.save_chat_message(session_id, "model", f"üöÄ Starting Application to **{job_url}**...")
        await log(f"Initializing application agent for: {job_url}")

    # Cloud Dispatch Check
    cloud_url = os.getenv("CLOUD_RUN_URL")
    # Only dispatch if we are not ALREADY in the cloud worker (prevent infinite loop if env vars are confusing)
    # But user_profile needs to be passed carefully.
    if cloud_url and not os.getenv("IS_CLOUD_WORKER") and execution_mode == 'cloud_run':
        import httpx
        print(f"üöÄ Dispatching Applier task to Cloud Worker: {cloud_url}")
        await log("Dispatching Applier to Cloud Worker...")
        
        try:
            async with httpx.AsyncClient() as client:
                payload = {
                    "type": "apply",
                    "job_url": job_url,
                    "resume_path": resume_path, # Note: This path is local, we might need to handle this differently for cloud. 
                    # actually worker endpoint logic below handles download. We pass resume_filename instead of path if possible?
                    # The worker needs to re-download. So we should pass resume_filename and user_id in payload, NOT local path.
                    "user_profile": user_profile,
                    "api_key": api_key,
                    "resume_filename": resume_filename,
                    "execution_mode": "local", # Worker runs it as local relative to itself (headless default), UNLESS we want worker to use managed?
                    # Actually if user wants Cloud Run + Managed Browser, we don't support that combo yet in UI.
                    # UI has "Google Cloud Run" (self-hosted) OR "Browser Use Cloud" (managed).
                    # "Google Cloud Run" implies self-hosted headless.
                    "session_id": session_id,
                    "instructions": instructions
                }
                headers = {"x-worker-secret": os.getenv("WORKER_SECRET", "")}
                resp = await client.post(f"{cloud_url}/api/worker/task", json=payload, headers=headers, timeout=10.0)
                if resp.status_code != 200:
                    print(f"‚ùå Cloud Dispatch Failed: {resp.text}")
                else:
                    return # Successfully dispatched
        except Exception as e:
             print(f"‚ùå Cloud Dispatch Error: {traceback.format_exc()}")

    # Resolve Lead ID for status updates
    user_id = user_profile.get("user_id") or user_profile.get("id")
    
    lead_id = None
    if user_id:
        # Use new method that handles fetch properly
        lead = supabase_service.get_lead_by_url(user_id, job_url)
        if lead:
            lead_id = lead['id']
            print(f"üìã Found Lead ID: {lead_id}")
            # Use specific ID update
            # Pass invalidation metadata if we have it
            supabase_service.update_lead_status(lead_id, "APPLYING", user_id=user_id, resume_filename=resume_filename)
        else:
            print("‚ö†Ô∏è Could not find existing lead for this URL. Status updates will be skipped.")
    else:
        print("‚ö†Ô∏è No User ID found in profile. Cannot resolve lead.")

    try:
        # Detect environment for headless mode
        is_headless = os.getenv("HEADLESS", "false").lower() == "true" or os.getenv("GITHUB_ACTIONS") == "true"
        applier = ApplierAgent(api_key=api_key, headless=is_headless)
        if execution_mode == 'browser_use_cloud':
            use_managed_browser = True
        else:
            use_managed_browser = False
        
        await log(f"Launching browser (Headless={is_headless}, Managed Cloud={use_managed_browser})...")
        
        # Pass lead_id and instructions to apply method
        result_status = await applier.apply(job_url, user_profile, resume_path, lead_id=lead_id, use_managed_browser=use_managed_browser, session_id=session_id, instructions=instructions)
        
        print(f"üèÅ Worker: Applier finished: {result_status}")
        
        await log(f"Application finished: {result_status}", type="complete")
        if session_id:
            supabase_service.save_chat_message(session_id, "model", f"üèÅ Application Finished. Status: **{result_status}**")
        
        if lead_id:
             # Default to FAILED safest
             final_status = "FAILED"
             
             # 1. Trust JSON status if available and valid
             if isinstance(result_status, str) and result_status in ["APPLIED", "SUBMITTED", "SUCCESS"]:
                 final_status = "APPLIED"
             elif "Submitted" in str(result_status) or "Success" in str(result_status):
                 final_status = "APPLIED"
             
             # 2. Overrule if explicit failure is detected
             if "FAIL" in str(result_status).upper() or "ERROR" in str(result_status).upper() or "COULD NOT BE FULLY COMPLETED" in str(result_status).upper():
                 final_status = "FAILED"
                 
             if "DryRun" in str(result_status): final_status = "DRY_RUN"
             
             print(f"‚úÖ Final Status Verdict: {final_status}")
             supabase_service.update_lead_status(lead_id, final_status, user_id=user_id, resume_filename=resume_filename)

    except asyncio.CancelledError:
        print(f"üõë Worker: Applier Cancelled for {job_url}")
        if session_id:
            supabase_service.save_chat_message(session_id, "model", "üõë Application Cancelled by user.")
        if lead_id:
            supabase_service.update_lead_status(lead_id, "CANCELLED", user_id=user_id, resume_filename=resume_filename)

    except Exception as e:
        print(f"‚ùå Worker: Applier Failed: {e}")
        if lead_id:
            supabase_service.update_lead_status(lead_id, "FAILED", user_id=user_id, resume_filename=resume_filename)
